CS122 Assignment 3 - Table Statistics and Plan Costing - Design Document
========================================================================

A:  Logistics
-------------

A1.  List your team name and the people who worked on this assignment.

     nanodb-ddj

     Daniel Wang
     Daniel Kong
     Jerry Zhang

A2.  Specify the tag name and commit-hash of the Git version you are
     submitting for your assignment.  (You can list the commit hashes
     of your repository tags with this command:  git show-ref --tags)

     Tag name:     <tag>
     Commit hash:  <hash>

A3.  Specify any late tokens you are applying to this assignment, or
     "none" if no late tokens.

     none

A4.  Briefly describe what parts of the assignment each teammate focused on.
     Daniel Wang: Statistics collection, selectivity estimation
     Daniel Kong: Plan costing
     Jerry Zhang: Extra credit etc

B:  Statistics Collection
-------------------------

B1.  Using pseudocode, summarize the implementation of your HeapTupleFile
     analyze() function.

     Pseudocode given below:

analyze():
    collectors = list of ColumnStatsCollectors, one for each column
    set tupleCount, tupleBytes, and #dataPages to 0
    for each DBPage:
        increment #dataPages
        for each slot in the DBPage:
            get tuple in slot (if non-empty)
            increment tupleCount
            increase tupleBytes by tuple size
            for each column in the tuple:
                put column value into the collectors
    create table stats with average tuple size (tupleBytes / tupleCount),
            tupleCount, column stats, etc.
    save stats


C:  Plan Costing Implementation
-------------------------------

C1.  Briefly describe how you estimate the number of tuples and the cost
     of a file-scan plan node.  What factors does your cost include?

    - The number of tuples is the selectivity times the number of tuples in the
      table.
    - The CPU cost is the number of tuples in the table, since it has to iterate
      through every tuple.
    - Disk IOs is the number of blocks in the table.
    - Expected tuple size is the average tuple size in the table.

C2.  Same question as for C1, but for simple filter nodes.

    - The number of tuples is the selectivity times the number of tuples in the
      child node.
    - The CPU cost is the number of tuples in the child node plus the CPU cost
      of the child node (since it has to iterate through each tuple).
    - Disk IOs is the child's disk cost, since we assume everything fits in memory.
    - Expected tuple size is the same as the child's expected tuple size.

C3.  Same question as for C1, but for nested-loop joins.

    - The CPU cost is the number of tuples it must iterate through, so it is
      n_r * n_s plus the sum of the child CPU costs.
    - The disk IOs is the sum of the child disk IOs, since we assume everything
      is contained in memory.
    - The expected tuple size is the sum of the child tuple sizes.
    - The number of tuples depends on the join type:
      - For a cross join, it is just n_r * n_s
      - For an inner join, it is selectivity * n_r * n_s
      - For a left outer join, it is the inner join + n_r (we only need a
        reasonable upper bound)
      - For a semijoin or antijoin, it is just n_r, since that is an upper bound
        to the number of tuples returned.

D:  Costing SQL Queries
-----------------------

Answer these questions after you have loaded the stores-28K.sql data, and
have analyzed all of the tables in that schema.

D1.  Paste the output of running:  EXPLAIN SELECT * FROM cities;
     Do not include debug lines, just the output of the command itself.
Explain Plan:
    FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

    Estimated 254.000000 tuples with average size 23.787401
    Estimated number of block IOs:  1


D2.  What is the estimated number of tuples that will be produced by each
     of these queries:

     SELECT * FROM cities WHERE population > 1000000;
     Estimated 225.582245 tuples with average size 23.787401

     SELECT * FROM cities WHERE population > 5000000;
     Estimated 99.262199 tuples with average size 23.787401

     SELECT * FROM cities WHERE population > 8000000;
     Estimated 4.522162 tuples with average size 23.787401


     How many tuples does each query produce?
     9, 1, and 1 for 1000000, 5000000, and 8000000, respectively.

     Briefly explain the difference between the estimated number of tuples
     and the actual number of tuples for these queries.

     We make the assumption that population is uniformly distributed
     between the min and max values. This is clearly not true. There are
     many more cities with low population than high population, so the
     true number of tuples is much lower than we expect. Instead, there
     are just a few outliers that pop up when we run these queries.


D3.  Paste the output of running these commands:

     EXPLAIN SELECT store_id FROM stores, cities
     WHERE stores.city_id = cities.city_id AND
           cities.population > 1000000;

Explain Plan:
    Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.2, tupSize=36.8, cpuCost=1020030.3, blockIOs=2004]
        SimpleFilter[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.POPULATION > 1000000] cost=[tuples=1776.2, tupSize=36.8, cpuCost=1018254.0, blockIOs=2004]
            NestedLoops[no pred] cost=[tuples=508000.0, tupSize=36.8, cpuCost=510254.0, blockIOs=2004]
                FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

Estimated 1776.238159 tuples with average size 36.787399
Estimated number of block IOs:  2004


     EXPLAIN SELECT store_id FROM stores JOIN
                    (SELECT city_id FROM cities
                     WHERE population > 1000000) AS big_cities
                    ON stores.city_id = big_cities.city_id;

Explain Plan:
    Project[values:  [STORES.STORE_ID]] cost=[tuples=1776.2, tupSize=36.8, cpuCost=455674.3, blockIOs=2004]
        NestedLoops[pred:  STORES.CITY_ID == BIG_CITIES.CITY_ID] cost=[tuples=1776.2, tupSize=36.8, cpuCost=453898.1, blockIOs=2004]
            FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
            Rename[resultTableName=BIG_CITIES] cost=[tuples=225.6, tupSize=23.8, cpuCost=733.6, blockIOs=1]
                Project[values:  [CITIES.CITY_ID]] cost=[tuples=225.6, tupSize=23.8, cpuCost=733.6, blockIOs=1]
                    SimpleFilter[pred:  CITIES.POPULATION > 1000000] cost=[tuples=225.6, tupSize=23.8, cpuCost=508.0, blockIOs=1]
                        FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]

Estimated 1776.238159 tuples with average size 36.787399
Estimated number of block IOs:  2004

     The estimated number of tuples produced should be the same, but the
     costs should be different.  Explain why.

     The costs are different because the first one does a cross join
     (generating the full set of tuples) and then applies a select predicate to
     the result of the cross join. This generates many more temporary rows than
     filtering as soon as possible and using a theta join instead of a cross
     join.

D4.  The assignment gives this example "slow" query:

     SELECT store_id, property_costs
     FROM stores, cities, states
     WHERE stores.city_id = cities.city_id AND
           cities.state_id = states.state_id AND
           state_name = 'Oregon' AND property_costs > 500000;

     How long does this query take to run, in seconds?

     This query took about 15 seconds to run (my computer has a SSD).

     Include the EXPLAIN output for the above query here.

Explain Plan:
    Project[values:  [STORES.STORE_ID, STORES.PROPERTY_COSTS]] cost=[tuples=19.6, tupSize=52.5, cpuCost=52326324.0, blockIOs=510004]
        SimpleFilter[pred:  STORES.CITY_ID == CITIES.CITY_ID AND CITIES.STATE_ID == STATES.STATE_ID AND STATES.STATE_NAME == 'Oregon' AND STORES.PROPERTY_COSTS > 500000] cost=[tuples=19.6, tupSize=52.5, cpuCost=52326304.0, blockIOs=510004]
            NestedLoops[no pred] cost=[tuples=25908000.0, tupSize=52.5, cpuCost=26418304.0, blockIOs=510004]
                NestedLoops[no pred] cost=[tuples=508000.0, tupSize=36.8, cpuCost=510254.0, blockIOs=2004]
                    FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]
                    FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                FileScan[table:  STATES] cost=[tuples=51.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]

Estimated 19.588217 tuples with average size 52.454067
Estimated number of block IOs:  510004

     How would you rewrite this query (e.g. using ON clauses, subqueries
     in the FROM clause, etc.) to be as optimal as possible?  Also include
     the result of EXPLAINing your query.

    SELECT store_id, property_costs
    FROM cities
      NATURAL JOIN (SELECT * FROM states WHERE state_name = 'Oregon') AS sta
      NATURAL JOIN (SELECT * FROM stores WHERE property_costs > 500000) AS sto;

Explain Plan:
    Project[values:  [STO.STORE_ID, STO.PROPERTY_COSTS]] cost=[tuples=19.6, tupSize=52.5, cpuCost=9629.6, blockIOs=274]
        Project[values:  [CITIES.CITY_ID AS CITY_ID, STATE_ID, CITIES.CITY_NAME, CITIES.POPULATION, STA.STATE_NAME, STO.STORE_ID, STO.PROPERTY_COSTS]] cost=[tuples=19.6, tupSize=52.5, cpuCost=9610.0, blockIOs=274]
            NestedLoops[pred:  CITIES.CITY_ID == STO.CITY_ID] cost=[tuples=19.6, tupSize=52.5, cpuCost=9590.4, blockIOs=274]
                Project[values:  [CITIES.STATE_ID AS STATE_ID, CITIES.CITY_ID, CITIES.CITY_NAME, CITIES.POPULATION, STA.STATE_NAME]] cost=[tuples=5.0, tupSize=39.5, cpuCost=615.0, blockIOs=255]
                    NestedLoops[pred:  CITIES.STATE_ID == STA.STATE_ID] cost=[tuples=5.0, tupSize=39.5, cpuCost=610.0, blockIOs=255]
                        FileScan[table:  CITIES] cost=[tuples=254.0, tupSize=23.8, cpuCost=254.0, blockIOs=1]
                        Rename[resultTableName=STA] cost=[tuples=1.0, tupSize=15.7, cpuCost=102.0, blockIOs=1]
                            SimpleFilter[pred:  STATES.STATE_NAME == 'Oregon'] cost=[tuples=1.0, tupSize=15.7, cpuCost=102.0, blockIOs=1]
                                FileScan[table:  STATES] cost=[tuples=51.0, tupSize=15.7, cpuCost=51.0, blockIOs=1]
                Rename[resultTableName=STO] cost=[tuples=999.0, tupSize=13.0, cpuCost=4000.0, blockIOs=4]
                    SimpleFilter[pred:  STORES.PROPERTY_COSTS > 500000] cost=[tuples=999.0, tupSize=13.0, cpuCost=4000.0, blockIOs=4]
                        FileScan[table:  STORES] cost=[tuples=2000.0, tupSize=13.0, cpuCost=2000.0, blockIOs=4]

Estimated 19.588217 tuples with average size 52.454067
Estimated number of block IOs:  274

This query takes about 0.1 seconds to evaluate.


E:  Extra Credit [OPTIONAL]
---------------------------

If you implemented any extra-credit tasks for this assignment, describe
them here.  The description should be like this, with stuff in "<>" replaced.
(The value i starts at 1 and increments...)

E<i>:  <one-line description>

     <brief summary of what you did, including the specific classes that
     we should look at for your implementation>

     <brief summary of test-cases that demonstrate/exercise your extra work>

E1: LimitOffsetNode and tests

    Implemented LimitOffsetNode, TestLimitOffset, and added planning for limit/offset
    to SimplePlanner. Database now supports the LIMIT and OFFSET clause, allowing
    querys to scroll through lengthy results a few at a time.

    A few simple test causes test that LIMIT clause reduces output as expected,
    that offset properly starts at the offset, and that offset going past the end
    of the table does not cause errors.

F:  Feedback [OPTIONAL]
-----------------------

These questions are optional, and they obviously won't affect your grade
in any way (including if you hate everything about the assignment and
databases in general, or Donnie and the TAs in particular).

NOTE:  If you wish to give anonymous feedback, a similar survey will be
       made available on the Moodle.

F1.  How many hours total did your team spend on this assignment?
     (That is, the sum of each teammate's time spent on the assignment.)

F2.  What parts of the assignment were most time-consuming?

F3.  Which parts of the assignment did you particularly enjoy?

F4.  Which parts did you particularly dislike?

F5.  Do you have any suggestions for how future versions of the
     assignment can be improved?
